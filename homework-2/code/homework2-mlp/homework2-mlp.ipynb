{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework-2: MLP for MNIST Classification\n",
    "\n",
    "### In this homework, you need to\n",
    "- #### implement SGD optimizer (`./optimizer.py`)\n",
    "- #### implement forward and backward for FCLayer (`layers/fc_layer.py`)\n",
    "- #### implement forward and backward for SigmoidLayer (`layers/sigmoid_layer.py`)\n",
    "- #### implement forward and backward for ReLULayer (`layers/relu_layer.py`)\n",
    "- #### implement EuclideanLossLayer (`criterion/euclidean_loss.py`)\n",
    "- #### implement SoftmaxCrossEntropyLossLayer (`criterion/softmax_cross_entropy.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset\n",
    "We use tensorflow tools to load dataset for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # Normalize from [0, 255.] to [0., 1.0], and then subtract by the mean value\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [784])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # Encode label with one-hot encoding\n",
    "    return tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyerparameters\n",
    "You can modify hyerparameters by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 10 #20\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 0.001 #0.001\n",
    "weight_decay = 0.1\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MLP with Euclidean Loss\n",
    "In part-1, you need to train a MLP with **Euclidean Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **./optimizer.py** and **criterion/euclidean_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import EuclideanLossLayer\n",
    "from optimizer import SGD\n",
    "\n",
    "criterion = EuclideanLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 MLP with Euclidean Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/fc_layer.py** and **layers/sigmoid_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import FCLayer, SigmoidLayer\n",
    "\n",
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/xunathan/Documents/清华大学/Deep Learning/Homework/homework-2/homework2-mlp/solver.py:15: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 3.8501\t Accuracy 0.1100\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 1.4683\t Accuracy 0.0880\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 1.0756\t Accuracy 0.0835\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 0.9302\t Accuracy 0.0881\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 0.8539\t Accuracy 0.0905\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 0.8035\t Accuracy 0.0957\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 0.7675\t Accuracy 0.0995\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 0.7397\t Accuracy 0.1049\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 0.7171\t Accuracy 0.1110\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 0.6979\t Accuracy 0.1175\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 0.6821\t Accuracy 0.1234\n",
      "\n",
      "Epoch [0]\t Average training loss 0.6672\t Average training accuracy 0.1314\n",
      "Epoch [0]\t Average validation loss 0.5107\t Average validation accuracy 0.2098\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 0.4915\t Accuracy 0.2100\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 0.5024\t Accuracy 0.2325\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 0.4984\t Accuracy 0.2435\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 0.4940\t Accuracy 0.2541\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 0.4910\t Accuracy 0.2580\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 0.4865\t Accuracy 0.2678\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 0.4824\t Accuracy 0.2750\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 0.4792\t Accuracy 0.2812\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 0.4755\t Accuracy 0.2890\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 0.4720\t Accuracy 0.2965\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 0.4690\t Accuracy 0.3025\n",
      "\n",
      "Epoch [1]\t Average training loss 0.4651\t Average training accuracy 0.3108\n",
      "Epoch [1]\t Average validation loss 0.4205\t Average validation accuracy 0.4014\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 0.4057\t Accuracy 0.4700\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 0.4178\t Accuracy 0.4163\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 0.4161\t Accuracy 0.4222\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 0.4144\t Accuracy 0.4266\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 0.4134\t Accuracy 0.4255\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 0.4110\t Accuracy 0.4307\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 0.4089\t Accuracy 0.4327\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 0.4077\t Accuracy 0.4357\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 0.4057\t Accuracy 0.4402\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 0.4039\t Accuracy 0.4449\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 0.4024\t Accuracy 0.4478\n",
      "\n",
      "Epoch [2]\t Average training loss 0.4001\t Average training accuracy 0.4532\n",
      "Epoch [2]\t Average validation loss 0.3708\t Average validation accuracy 0.5320\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 0.3590\t Accuracy 0.5900\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 0.3711\t Accuracy 0.5229\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 0.3704\t Accuracy 0.5240\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 0.3700\t Accuracy 0.5228\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 0.3699\t Accuracy 0.5223\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 0.3685\t Accuracy 0.5252\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 0.3673\t Accuracy 0.5263\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 0.3670\t Accuracy 0.5274\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 0.3659\t Accuracy 0.5299\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 0.3649\t Accuracy 0.5318\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 0.3641\t Accuracy 0.5332\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3626\t Average training accuracy 0.5373\n",
      "Epoch [3]\t Average validation loss 0.3407\t Average validation accuracy 0.6088\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 0.3310\t Accuracy 0.6300\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 0.3429\t Accuracy 0.5876\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 0.3426\t Accuracy 0.5865\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 0.3430\t Accuracy 0.5851\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 0.3433\t Accuracy 0.5850\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 0.3423\t Accuracy 0.5865\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 0.3416\t Accuracy 0.5875\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 0.3419\t Accuracy 0.5880\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 0.3412\t Accuracy 0.5899\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 0.3407\t Accuracy 0.5912\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 0.3403\t Accuracy 0.5924\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3392\t Average training accuracy 0.5957\n",
      "Epoch [4]\t Average validation loss 0.3214\t Average validation accuracy 0.6598\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 0.3133\t Accuracy 0.6500\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 0.3248\t Accuracy 0.6306\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 0.3248\t Accuracy 0.6333\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 0.3256\t Accuracy 0.6305\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 0.3261\t Accuracy 0.6306\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 0.3255\t Accuracy 0.6314\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 0.3251\t Accuracy 0.6325\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 0.3257\t Accuracy 0.6325\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 0.3252\t Accuracy 0.6337\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 0.3250\t Accuracy 0.6348\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 0.3248\t Accuracy 0.6357\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3240\t Average training accuracy 0.6383\n",
      "Epoch [5]\t Average validation loss 0.3088\t Average validation accuracy 0.6980\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 0.3018\t Accuracy 0.6900\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 0.3130\t Accuracy 0.6657\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 0.3131\t Accuracy 0.6666\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 0.3142\t Accuracy 0.6648\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 0.3148\t Accuracy 0.6644\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 0.3144\t Accuracy 0.6643\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 0.3142\t Accuracy 0.6651\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 0.3150\t Accuracy 0.6647\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 0.3146\t Accuracy 0.6657\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 0.3146\t Accuracy 0.6663\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 0.3145\t Accuracy 0.6670\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3139\t Average training accuracy 0.6693\n",
      "Epoch [6]\t Average validation loss 0.3004\t Average validation accuracy 0.7224\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 0.2943\t Accuracy 0.7200\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 0.3053\t Accuracy 0.6924\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 0.3054\t Accuracy 0.6938\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 0.3067\t Accuracy 0.6915\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 0.3074\t Accuracy 0.6901\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 0.3071\t Accuracy 0.6901\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 0.3070\t Accuracy 0.6905\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 0.3079\t Accuracy 0.6897\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 0.3077\t Accuracy 0.6903\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 0.3078\t Accuracy 0.6912\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 0.3078\t Accuracy 0.6918\n",
      "\n",
      "Epoch [7]\t Average training loss 0.3073\t Average training accuracy 0.6938\n",
      "Epoch [7]\t Average validation loss 0.2951\t Average validation accuracy 0.7434\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 0.2896\t Accuracy 0.7200\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 0.3003\t Accuracy 0.7094\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 0.3004\t Accuracy 0.7119\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 0.3019\t Accuracy 0.7090\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 0.3026\t Accuracy 0.7075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 0.3024\t Accuracy 0.7077\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 0.3024\t Accuracy 0.7077\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 0.3034\t Accuracy 0.7070\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 0.3032\t Accuracy 0.7074\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 0.3034\t Accuracy 0.7080\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 0.3035\t Accuracy 0.7083\n",
      "\n",
      "Epoch [8]\t Average training loss 0.3031\t Average training accuracy 0.7099\n",
      "Epoch [8]\t Average validation loss 0.2918\t Average validation accuracy 0.7578\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 0.2869\t Accuracy 0.7300\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 0.2974\t Accuracy 0.7257\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 0.2975\t Accuracy 0.7281\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 0.2990\t Accuracy 0.7236\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 0.2998\t Accuracy 0.7218\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 0.2997\t Accuracy 0.7216\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 0.2997\t Accuracy 0.7212\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 0.3007\t Accuracy 0.7200\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 0.3007\t Accuracy 0.7206\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 0.3009\t Accuracy 0.7210\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 0.3010\t Accuracy 0.7211\n",
      "\n",
      "Epoch [9]\t Average training loss 0.3007\t Average training accuracy 0.7228\n",
      "Epoch [9]\t Average validation loss 0.2902\t Average validation accuracy 0.7680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.7447.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 MLP with Euclidean Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/relu_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import ReLULayer\n",
    "\n",
    "reluMLP = Network()\n",
    "# TODO build ReLUMLP with FCLayer and ReLULayer\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 3.1825\t Accuracy 0.1200\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 1.9200\t Accuracy 0.1484\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 1.6409\t Accuracy 0.1621\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 1.4879\t Accuracy 0.1713\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 1.3766\t Accuracy 0.1883\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 1.2880\t Accuracy 0.2033\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 1.2158\t Accuracy 0.2204\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 1.1571\t Accuracy 0.2344\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 1.1069\t Accuracy 0.2470\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 1.0645\t Accuracy 0.2567\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 1.0267\t Accuracy 0.2697\n",
      "\n",
      "Epoch [0]\t Average training loss 0.9928\t Average training accuracy 0.2813\n",
      "Epoch [0]\t Average validation loss 0.6215\t Average validation accuracy 0.4134\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 0.5964\t Accuracy 0.4000\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 0.6091\t Accuracy 0.4155\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 0.6006\t Accuracy 0.4244\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 0.5965\t Accuracy 0.4250\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 0.5886\t Accuracy 0.4339\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 0.5786\t Accuracy 0.4415\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 0.5690\t Accuracy 0.4495\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 0.5612\t Accuracy 0.4552\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 0.5532\t Accuracy 0.4618\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 0.5465\t Accuracy 0.4654\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 0.5398\t Accuracy 0.4714\n",
      "\n",
      "Epoch [1]\t Average training loss 0.5326\t Average training accuracy 0.4777\n",
      "Epoch [1]\t Average validation loss 0.4423\t Average validation accuracy 0.5640\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 0.4213\t Accuracy 0.5600\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 0.4413\t Accuracy 0.5580\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 0.4383\t Accuracy 0.5633\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 0.4395\t Accuracy 0.5553\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 0.4373\t Accuracy 0.5597\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 0.4331\t Accuracy 0.5622\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 0.4289\t Accuracy 0.5662\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 0.4260\t Accuracy 0.5685\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 0.4224\t Accuracy 0.5725\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 0.4196\t Accuracy 0.5745\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 0.4167\t Accuracy 0.5782\n",
      "\n",
      "Epoch [2]\t Average training loss 0.4130\t Average training accuracy 0.5825\n",
      "Epoch [2]\t Average validation loss 0.3610\t Average validation accuracy 0.6520\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 0.3431\t Accuracy 0.6400\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 0.3636\t Accuracy 0.6441\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 0.3624\t Accuracy 0.6511\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 0.3650\t Accuracy 0.6415\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 0.3645\t Accuracy 0.6425\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 0.3621\t Accuracy 0.6426\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 0.3599\t Accuracy 0.6432\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 0.3587\t Accuracy 0.6434\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 0.3565\t Accuracy 0.6461\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 0.3551\t Accuracy 0.6472\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 0.3535\t Accuracy 0.6498\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3512\t Average training accuracy 0.6532\n",
      "Epoch [3]\t Average validation loss 0.3134\t Average validation accuracy 0.7176\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 0.2979\t Accuracy 0.7100\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 0.3179\t Accuracy 0.7039\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 0.3175\t Accuracy 0.7066\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 0.3207\t Accuracy 0.6951\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 0.3209\t Accuracy 0.6950\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 0.3193\t Accuracy 0.6946\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 0.3180\t Accuracy 0.6948\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 0.3177\t Accuracy 0.6936\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 0.3162\t Accuracy 0.6957\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 0.3155\t Accuracy 0.6965\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 0.3145\t Accuracy 0.6983\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3128\t Average training accuracy 0.7011\n",
      "Epoch [4]\t Average validation loss 0.2820\t Average validation accuracy 0.7606\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 0.2683\t Accuracy 0.8000\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 0.2876\t Accuracy 0.7494\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 0.2879\t Accuracy 0.7482\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 0.2912\t Accuracy 0.7370\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 0.2917\t Accuracy 0.7360\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 0.2906\t Accuracy 0.7351\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 0.2899\t Accuracy 0.7340\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 0.2900\t Accuracy 0.7325\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 0.2889\t Accuracy 0.7340\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 0.2885\t Accuracy 0.7336\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 0.2879\t Accuracy 0.7351\n",
      "\n",
      "Epoch [5]\t Average training loss 0.2866\t Average training accuracy 0.7376\n",
      "Epoch [5]\t Average validation loss 0.2598\t Average validation accuracy 0.7914\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 0.2479\t Accuracy 0.8200\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 0.2663\t Accuracy 0.7761\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 0.2668\t Accuracy 0.7748\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 0.2703\t Accuracy 0.7650\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 0.2710\t Accuracy 0.7633\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 0.2701\t Accuracy 0.7623\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 0.2697\t Accuracy 0.7612\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 0.2701\t Accuracy 0.7600\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 0.2692\t Accuracy 0.7611\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 0.2691\t Accuracy 0.7604\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 0.2687\t Accuracy 0.7616\n",
      "\n",
      "Epoch [6]\t Average training loss 0.2677\t Average training accuracy 0.7635\n",
      "Epoch [6]\t Average validation loss 0.2434\t Average validation accuracy 0.8154\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 0.2332\t Accuracy 0.8200\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 0.2505\t Accuracy 0.8004\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 0.2513\t Accuracy 0.7967\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 0.2547\t Accuracy 0.7853\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 0.2555\t Accuracy 0.7844\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 0.2548\t Accuracy 0.7830\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 0.2546\t Accuracy 0.7816\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 0.2552\t Accuracy 0.7801\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 0.2545\t Accuracy 0.7814\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 0.2546\t Accuracy 0.7813\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 0.2543\t Accuracy 0.7818\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2534\t Average training accuracy 0.7830\n",
      "Epoch [7]\t Average validation loss 0.2310\t Average validation accuracy 0.8330\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 0.2220\t Accuracy 0.8300\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 0.2385\t Accuracy 0.8120\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 0.2394\t Accuracy 0.8107\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 0.2429\t Accuracy 0.8015\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 0.2437\t Accuracy 0.7993\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 0.2432\t Accuracy 0.7980\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 0.2431\t Accuracy 0.7968\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 0.2438\t Accuracy 0.7954\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 0.2433\t Accuracy 0.7971\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 0.2435\t Accuracy 0.7967\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 0.2433\t Accuracy 0.7972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.2425\t Average training accuracy 0.7983\n",
      "Epoch [8]\t Average validation loss 0.2214\t Average validation accuracy 0.8464\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 0.2133\t Accuracy 0.8500\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 0.2293\t Accuracy 0.8253\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 0.2303\t Accuracy 0.8225\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 0.2338\t Accuracy 0.8128\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 0.2346\t Accuracy 0.8109\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 0.2342\t Accuracy 0.8095\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 0.2342\t Accuracy 0.8081\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 0.2351\t Accuracy 0.8071\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 0.2346\t Accuracy 0.8083\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 0.2349\t Accuracy 0.8079\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 0.2348\t Accuracy 0.8083\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2342\t Average training accuracy 0.8093\n",
      "Epoch [9]\t Average validation loss 0.2141\t Average validation accuracy 0.8550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.8291.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLP with Softmax Cross-Entropy Loss\n",
    "In part-2, you need to train a MLP with **Softmax Cross-Entropy Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively again.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **criterion/softmax_cross_entropy_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MLP with Softmax Cross-Entropy Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 2.6216\t Accuracy 0.1100\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 2.5924\t Accuracy 0.0904\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 2.5555\t Accuracy 0.0952\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 2.5213\t Accuracy 0.0979\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 2.4944\t Accuracy 0.1012\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 2.4727\t Accuracy 0.1010\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 2.4527\t Accuracy 0.1021\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 2.4362\t Accuracy 0.1035\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 2.4220\t Accuracy 0.1049\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 2.4097\t Accuracy 0.1055\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 2.3975\t Accuracy 0.1074\n",
      "\n",
      "Epoch [0]\t Average training loss 2.3865\t Average training accuracy 0.1102\n",
      "Epoch [0]\t Average validation loss 2.2638\t Average validation accuracy 0.1578\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 2.2599\t Accuracy 0.1500\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 2.2676\t Accuracy 0.1433\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 2.2597\t Accuracy 0.1519\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 2.2527\t Accuracy 0.1601\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 2.2475\t Accuracy 0.1698\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 2.2427\t Accuracy 0.1788\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 2.2368\t Accuracy 0.1906\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 2.2329\t Accuracy 0.1983\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 2.2288\t Accuracy 0.2068\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 2.2253\t Accuracy 0.2135\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 2.2210\t Accuracy 0.2235\n",
      "\n",
      "Epoch [1]\t Average training loss 2.2167\t Average training accuracy 0.2334\n",
      "Epoch [1]\t Average validation loss 2.1599\t Average validation accuracy 0.3492\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 2.1572\t Accuracy 0.3400\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 2.1675\t Accuracy 0.3447\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 2.1615\t Accuracy 0.3570\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 2.1571\t Accuracy 0.3675\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 2.1540\t Accuracy 0.3767\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 2.1505\t Accuracy 0.3856\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 2.1460\t Accuracy 0.3946\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 2.1438\t Accuracy 0.3992\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 2.1409\t Accuracy 0.4050\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 2.1387\t Accuracy 0.4098\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 2.1357\t Accuracy 0.4165\n",
      "\n",
      "Epoch [2]\t Average training loss 2.1324\t Average training accuracy 0.4229\n",
      "Epoch [2]\t Average validation loss 2.0848\t Average validation accuracy 0.5168\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 2.0816\t Accuracy 0.5200\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 2.0946\t Accuracy 0.5039\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 2.0897\t Accuracy 0.5039\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 2.0868\t Accuracy 0.5052\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 2.0849\t Accuracy 0.5101\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 2.0822\t Accuracy 0.5138\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 2.0785\t Accuracy 0.5178\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 2.0776\t Accuracy 0.5179\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 2.0753\t Accuracy 0.5209\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 2.0740\t Accuracy 0.5230\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 2.0718\t Accuracy 0.5272\n",
      "\n",
      "Epoch [3]\t Average training loss 2.0693\t Average training accuracy 0.5307\n",
      "Epoch [3]\t Average validation loss 2.0277\t Average validation accuracy 0.6000\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 2.0240\t Accuracy 0.6700\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 2.0390\t Accuracy 0.5800\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 2.0351\t Accuracy 0.5779\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 2.0333\t Accuracy 0.5751\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 2.0323\t Accuracy 0.5773\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 2.0303\t Accuracy 0.5798\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 2.0271\t Accuracy 0.5825\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 2.0271\t Accuracy 0.5812\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 2.0254\t Accuracy 0.5837\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 2.0248\t Accuracy 0.5849\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 2.0231\t Accuracy 0.5882\n",
      "\n",
      "Epoch [4]\t Average training loss 2.0212\t Average training accuracy 0.5905\n",
      "Epoch [4]\t Average validation loss 1.9843\t Average validation accuracy 0.6498\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 1.9802\t Accuracy 0.6900\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 1.9967\t Accuracy 0.6271\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 1.9936\t Accuracy 0.6243\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 1.9927\t Accuracy 0.6195\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 1.9923\t Accuracy 0.6200\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 1.9908\t Accuracy 0.6224\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 1.9881\t Accuracy 0.6241\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 1.9888\t Accuracy 0.6220\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 1.9875\t Accuracy 0.6241\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 1.9874\t Accuracy 0.6243\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 1.9862\t Accuracy 0.6264\n",
      "\n",
      "Epoch [5]\t Average training loss 1.9847\t Average training accuracy 0.6279\n",
      "Epoch [5]\t Average validation loss 1.9515\t Average validation accuracy 0.6838\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 1.9470\t Accuracy 0.7100\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 1.9647\t Accuracy 0.6531\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 1.9621\t Accuracy 0.6500\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 1.9620\t Accuracy 0.6454\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 1.9621\t Accuracy 0.6448\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 1.9609\t Accuracy 0.6467\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 1.9586\t Accuracy 0.6482\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 1.9599\t Accuracy 0.6462\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 1.9589\t Accuracy 0.6483\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 1.9592\t Accuracy 0.6481\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 1.9583\t Accuracy 0.6494\n",
      "\n",
      "Epoch [6]\t Average training loss 1.9572\t Average training accuracy 0.6506\n",
      "Epoch [6]\t Average validation loss 1.9269\t Average validation accuracy 0.7030\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 1.9221\t Accuracy 0.7400\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 1.9405\t Accuracy 0.6680\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 1.9385\t Accuracy 0.6652\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 1.9389\t Accuracy 0.6615\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 1.9394\t Accuracy 0.6605\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 1.9385\t Accuracy 0.6625\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 1.9365\t Accuracy 0.6639\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 1.9382\t Accuracy 0.6614\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 1.9374\t Accuracy 0.6634\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 1.9380\t Accuracy 0.6631\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 1.9374\t Accuracy 0.6643\n",
      "\n",
      "Epoch [7]\t Average training loss 1.9366\t Average training accuracy 0.6654\n",
      "Epoch [7]\t Average validation loss 1.9086\t Average validation accuracy 0.7122\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 1.9036\t Accuracy 0.7300\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 1.9226\t Accuracy 0.6780\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 1.9209\t Accuracy 0.6750\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 1.9218\t Accuracy 0.6715\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 1.9226\t Accuracy 0.6704\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 1.9219\t Accuracy 0.6722\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 1.9202\t Accuracy 0.6731\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 1.9222\t Accuracy 0.6710\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 1.9216\t Accuracy 0.6729\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 1.9224\t Accuracy 0.6728\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 1.9221\t Accuracy 0.6734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 1.9214\t Average training accuracy 0.6744\n",
      "Epoch [8]\t Average validation loss 1.8954\t Average validation accuracy 0.7192\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 1.8902\t Accuracy 0.7500\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 1.9095\t Accuracy 0.6865\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 1.9082\t Accuracy 0.6832\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 1.9094\t Accuracy 0.6793\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 1.9105\t Accuracy 0.6779\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 1.9099\t Accuracy 0.6793\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 1.9085\t Accuracy 0.6795\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 1.9107\t Accuracy 0.6774\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 1.9102\t Accuracy 0.6791\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 1.9112\t Accuracy 0.6791\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 1.9110\t Accuracy 0.6796\n",
      "\n",
      "Epoch [9]\t Average training loss 1.9105\t Average training accuracy 0.6807\n",
      "Epoch [9]\t Average validation loss 1.8862\t Average validation accuracy 0.7232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.7041.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 MLP with Softmax Cross-Entropy Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reluMLP = Network()\n",
    "# Build ReLUMLP with FCLayer and ReLULayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 2.5023\t Accuracy 0.1700\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 2.4209\t Accuracy 0.1404\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 2.3411\t Accuracy 0.1650\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 2.2730\t Accuracy 0.1899\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 2.2202\t Accuracy 0.2127\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 2.1682\t Accuracy 0.2405\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 2.1213\t Accuracy 0.2692\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 2.0810\t Accuracy 0.2936\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 2.0416\t Accuracy 0.3181\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 2.0072\t Accuracy 0.3412\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 1.9746\t Accuracy 0.3611\n",
      "\n",
      "Epoch [0]\t Average training loss 1.9424\t Average training accuracy 0.3800\n",
      "Epoch [0]\t Average validation loss 1.5509\t Average validation accuracy 0.6124\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 1.5342\t Accuracy 0.6400\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 1.5592\t Accuracy 0.6053\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 1.5361\t Accuracy 0.6169\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 1.5196\t Accuracy 0.6204\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 1.5086\t Accuracy 0.6242\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 1.4899\t Accuracy 0.6300\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 1.4729\t Accuracy 0.6361\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 1.4620\t Accuracy 0.6384\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 1.4459\t Accuracy 0.6443\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 1.4335\t Accuracy 0.6487\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 1.4204\t Accuracy 0.6530\n",
      "\n",
      "Epoch [1]\t Average training loss 1.4058\t Average training accuracy 0.6589\n",
      "Epoch [1]\t Average validation loss 1.1925\t Average validation accuracy 0.7534\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 1.1766\t Accuracy 0.7500\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 1.2240\t Accuracy 0.7253\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 1.2124\t Accuracy 0.7322\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 1.2079\t Accuracy 0.7296\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 1.2067\t Accuracy 0.7301\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 1.1963\t Accuracy 0.7314\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 1.1873\t Accuracy 0.7330\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 1.1850\t Accuracy 0.7339\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 1.1762\t Accuracy 0.7369\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 1.1707\t Accuracy 0.7384\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 1.1642\t Accuracy 0.7400\n",
      "\n",
      "Epoch [2]\t Average training loss 1.1558\t Average training accuracy 0.7432\n",
      "Epoch [2]\t Average validation loss 1.0039\t Average validation accuracy 0.8070\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 0.9950\t Accuracy 0.8000\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 1.0461\t Accuracy 0.7804\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 1.0410\t Accuracy 0.7825\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 1.0420\t Accuracy 0.7779\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 1.0446\t Accuracy 0.7768\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 1.0379\t Accuracy 0.7776\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 1.0327\t Accuracy 0.7782\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 1.0342\t Accuracy 0.7784\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 1.0290\t Accuracy 0.7801\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 1.0266\t Accuracy 0.7808\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 1.0232\t Accuracy 0.7813\n",
      "\n",
      "Epoch [3]\t Average training loss 1.0178\t Average training accuracy 0.7833\n",
      "Epoch [3]\t Average validation loss 0.8961\t Average validation accuracy 0.8410\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 0.8942\t Accuracy 0.8400\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 0.9431\t Accuracy 0.8090\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 0.9419\t Accuracy 0.8085\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 0.9458\t Accuracy 0.8042\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 0.9499\t Accuracy 0.8025\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 0.9451\t Accuracy 0.8026\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 0.9418\t Accuracy 0.8029\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 0.9453\t Accuracy 0.8028\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 0.9419\t Accuracy 0.8042\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 0.9412\t Accuracy 0.8048\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 0.9394\t Accuracy 0.8049\n",
      "\n",
      "Epoch [4]\t Average training loss 0.9357\t Average training accuracy 0.8063\n",
      "Epoch [4]\t Average validation loss 0.8303\t Average validation accuracy 0.8580\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 0.8349\t Accuracy 0.8400\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 0.8798\t Accuracy 0.8265\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 0.8810\t Accuracy 0.8256\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 0.8866\t Accuracy 0.8205\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 0.8913\t Accuracy 0.8182\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 0.8876\t Accuracy 0.8188\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 0.8854\t Accuracy 0.8190\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 0.8899\t Accuracy 0.8184\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 0.8876\t Accuracy 0.8195\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 0.8878\t Accuracy 0.8200\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 0.8870\t Accuracy 0.8198\n",
      "\n",
      "Epoch [5]\t Average training loss 0.8843\t Average training accuracy 0.8206\n",
      "Epoch [5]\t Average validation loss 0.7886\t Average validation accuracy 0.8690\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 0.7979\t Accuracy 0.8400\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 0.8393\t Accuracy 0.8341\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 0.8420\t Accuracy 0.8353\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 0.8487\t Accuracy 0.8299\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 0.8536\t Accuracy 0.8281\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 0.8506\t Accuracy 0.8292\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 0.8490\t Accuracy 0.8291\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 0.8541\t Accuracy 0.8282\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 0.8526\t Accuracy 0.8292\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 0.8533\t Accuracy 0.8293\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 0.8531\t Accuracy 0.8290\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8510\t Average training accuracy 0.8296\n",
      "Epoch [6]\t Average validation loss 0.7613\t Average validation accuracy 0.8766\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 0.7731\t Accuracy 0.8500\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 0.8128\t Accuracy 0.8425\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 0.8165\t Accuracy 0.8427\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 0.8239\t Accuracy 0.8368\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 0.8289\t Accuracy 0.8350\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 0.8262\t Accuracy 0.8361\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 0.8251\t Accuracy 0.8364\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 0.8306\t Accuracy 0.8355\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 0.8295\t Accuracy 0.8358\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 0.8306\t Accuracy 0.8358\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 0.8307\t Accuracy 0.8354\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8291\t Average training accuracy 0.8360\n",
      "Epoch [7]\t Average validation loss 0.7435\t Average validation accuracy 0.8814\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 0.7569\t Accuracy 0.8500\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 0.7953\t Accuracy 0.8500\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 0.7998\t Accuracy 0.8482\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 0.8077\t Accuracy 0.8432\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 0.8126\t Accuracy 0.8421\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 0.8102\t Accuracy 0.8429\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 0.8094\t Accuracy 0.8429\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 0.8151\t Accuracy 0.8415\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 0.8143\t Accuracy 0.8417\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 0.8157\t Accuracy 0.8415\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 0.8161\t Accuracy 0.8410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.8147\t Average training accuracy 0.8414\n",
      "Epoch [8]\t Average validation loss 0.7321\t Average validation accuracy 0.8836\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 0.7469\t Accuracy 0.8800\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 0.7840\t Accuracy 0.8557\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 0.7890\t Accuracy 0.8531\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 0.7972\t Accuracy 0.8486\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 0.8021\t Accuracy 0.8470\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 0.7999\t Accuracy 0.8475\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 0.7992\t Accuracy 0.8474\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 0.8051\t Accuracy 0.8459\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 0.8045\t Accuracy 0.8460\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 0.8061\t Accuracy 0.8458\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 0.8066\t Accuracy 0.8455\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8055\t Average training accuracy 0.8456\n",
      "Epoch [9]\t Average validation loss 0.7251\t Average validation accuracy 0.8874\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.8595.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~~You have finished homework2-mlp, congratulations!~~  \n",
    "\n",
    "**Next, according to the requirements 4) of report:**\n",
    "### **You need to construct a two-hidden-layer MLP, using any activation function and loss function.**\n",
    "\n",
    "**Note: Please insert some new cells blow (using '+' bottom in the toolbar) refer to above codes. Do not modify the former code directly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK HYPER-PARAMETERS\n",
    "batch_size = 100\n",
    "max_epoch = 10 #20\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 0.001 #0.001\n",
    "weight_decay = 0.1\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS CRITERIA\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "\n",
    "# OPTIMIZATION METHOD\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)\n",
    "\n",
    "# CREATE NETWORK\n",
    "reluMLP = Network()\n",
    "reluMLP.add(FCLayer(784, 456))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(456, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 2.6287\t Accuracy 0.0800\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 2.5262\t Accuracy 0.0859\n",
      "Epoch [0][10]\t Batch [100][550]\t Training Loss 2.4006\t Accuracy 0.1226\n",
      "Epoch [0][10]\t Batch [150][550]\t Training Loss 2.3123\t Accuracy 0.1614\n",
      "Epoch [0][10]\t Batch [200][550]\t Training Loss 2.2393\t Accuracy 0.2017\n",
      "Epoch [0][10]\t Batch [250][550]\t Training Loss 2.1759\t Accuracy 0.2395\n",
      "Epoch [0][10]\t Batch [300][550]\t Training Loss 2.1160\t Accuracy 0.2762\n",
      "Epoch [0][10]\t Batch [350][550]\t Training Loss 2.0701\t Accuracy 0.3055\n",
      "Epoch [0][10]\t Batch [400][550]\t Training Loss 2.0208\t Accuracy 0.3357\n",
      "Epoch [0][10]\t Batch [450][550]\t Training Loss 1.9799\t Accuracy 0.3608\n",
      "Epoch [0][10]\t Batch [500][550]\t Training Loss 1.9396\t Accuracy 0.3853\n",
      "\n",
      "Epoch [0]\t Average training loss 1.9005\t Average training accuracy 0.4084\n",
      "Epoch [0]\t Average validation loss 1.4369\t Average validation accuracy 0.6840\n",
      "\n",
      "Epoch [1][10]\t Batch [0][550]\t Training Loss 1.4448\t Accuracy 0.6900\n",
      "Epoch [1][10]\t Batch [50][550]\t Training Loss 1.4629\t Accuracy 0.6596\n",
      "Epoch [1][10]\t Batch [100][550]\t Training Loss 1.4339\t Accuracy 0.6719\n",
      "Epoch [1][10]\t Batch [150][550]\t Training Loss 1.4186\t Accuracy 0.6734\n",
      "Epoch [1][10]\t Batch [200][550]\t Training Loss 1.4026\t Accuracy 0.6797\n",
      "Epoch [1][10]\t Batch [250][550]\t Training Loss 1.3830\t Accuracy 0.6856\n",
      "Epoch [1][10]\t Batch [300][550]\t Training Loss 1.3625\t Accuracy 0.6928\n",
      "Epoch [1][10]\t Batch [350][550]\t Training Loss 1.3526\t Accuracy 0.6950\n",
      "Epoch [1][10]\t Batch [400][550]\t Training Loss 1.3340\t Accuracy 0.7013\n",
      "Epoch [1][10]\t Batch [450][550]\t Training Loss 1.3205\t Accuracy 0.7053\n",
      "Epoch [1][10]\t Batch [500][550]\t Training Loss 1.3057\t Accuracy 0.7091\n",
      "\n",
      "Epoch [1]\t Average training loss 1.2894\t Average training accuracy 0.7143\n",
      "Epoch [1]\t Average validation loss 1.0548\t Average validation accuracy 0.8000\n",
      "\n",
      "Epoch [2][10]\t Batch [0][550]\t Training Loss 1.0668\t Accuracy 0.8100\n",
      "Epoch [2][10]\t Batch [50][550]\t Training Loss 1.1030\t Accuracy 0.7753\n",
      "Epoch [2][10]\t Batch [100][550]\t Training Loss 1.0911\t Accuracy 0.7784\n",
      "Epoch [2][10]\t Batch [150][550]\t Training Loss 1.0891\t Accuracy 0.7752\n",
      "Epoch [2][10]\t Batch [200][550]\t Training Loss 1.0839\t Accuracy 0.7764\n",
      "Epoch [2][10]\t Batch [250][550]\t Training Loss 1.0744\t Accuracy 0.7778\n",
      "Epoch [2][10]\t Batch [300][550]\t Training Loss 1.0642\t Accuracy 0.7803\n",
      "Epoch [2][10]\t Batch [350][550]\t Training Loss 1.0636\t Accuracy 0.7798\n",
      "Epoch [2][10]\t Batch [400][550]\t Training Loss 1.0541\t Accuracy 0.7825\n",
      "Epoch [2][10]\t Batch [450][550]\t Training Loss 1.0488\t Accuracy 0.7839\n",
      "Epoch [2][10]\t Batch [500][550]\t Training Loss 1.0422\t Accuracy 0.7850\n",
      "\n",
      "Epoch [2]\t Average training loss 1.0336\t Average training accuracy 0.7866\n",
      "Epoch [2]\t Average validation loss 0.8752\t Average validation accuracy 0.8422\n",
      "\n",
      "Epoch [3][10]\t Batch [0][550]\t Training Loss 0.8938\t Accuracy 0.8400\n",
      "Epoch [3][10]\t Batch [50][550]\t Training Loss 0.9309\t Accuracy 0.8147\n",
      "Epoch [3][10]\t Batch [100][550]\t Training Loss 0.9269\t Accuracy 0.8145\n",
      "Epoch [3][10]\t Batch [150][550]\t Training Loss 0.9306\t Accuracy 0.8095\n",
      "Epoch [3][10]\t Batch [200][550]\t Training Loss 0.9296\t Accuracy 0.8102\n",
      "Epoch [3][10]\t Batch [250][550]\t Training Loss 0.9243\t Accuracy 0.8113\n",
      "Epoch [3][10]\t Batch [300][550]\t Training Loss 0.9187\t Accuracy 0.8128\n",
      "Epoch [3][10]\t Batch [350][550]\t Training Loss 0.9217\t Accuracy 0.8119\n",
      "Epoch [3][10]\t Batch [400][550]\t Training Loss 0.9163\t Accuracy 0.8136\n",
      "Epoch [3][10]\t Batch [450][550]\t Training Loss 0.9145\t Accuracy 0.8145\n",
      "Epoch [3][10]\t Batch [500][550]\t Training Loss 0.9116\t Accuracy 0.8150\n",
      "\n",
      "Epoch [3]\t Average training loss 0.9064\t Average training accuracy 0.8159\n",
      "Epoch [3]\t Average validation loss 0.7825\t Average validation accuracy 0.8638\n",
      "\n",
      "Epoch [4][10]\t Batch [0][550]\t Training Loss 0.8062\t Accuracy 0.8500\n",
      "Epoch [4][10]\t Batch [50][550]\t Training Loss 0.8406\t Accuracy 0.8376\n",
      "Epoch [4][10]\t Batch [100][550]\t Training Loss 0.8405\t Accuracy 0.8354\n",
      "Epoch [4][10]\t Batch [150][550]\t Training Loss 0.8466\t Accuracy 0.8297\n",
      "Epoch [4][10]\t Batch [200][550]\t Training Loss 0.8475\t Accuracy 0.8295\n",
      "Epoch [4][10]\t Batch [250][550]\t Training Loss 0.8439\t Accuracy 0.8302\n",
      "Epoch [4][10]\t Batch [300][550]\t Training Loss 0.8405\t Accuracy 0.8310\n",
      "Epoch [4][10]\t Batch [350][550]\t Training Loss 0.8452\t Accuracy 0.8297\n",
      "Epoch [4][10]\t Batch [400][550]\t Training Loss 0.8419\t Accuracy 0.8314\n",
      "Epoch [4][10]\t Batch [450][550]\t Training Loss 0.8417\t Accuracy 0.8316\n",
      "Epoch [4][10]\t Batch [500][550]\t Training Loss 0.8405\t Accuracy 0.8316\n",
      "\n",
      "Epoch [4]\t Average training loss 0.8372\t Average training accuracy 0.8321\n",
      "Epoch [4]\t Average validation loss 0.7312\t Average validation accuracy 0.8762\n",
      "\n",
      "Epoch [5][10]\t Batch [0][550]\t Training Loss 0.7581\t Accuracy 0.8600\n",
      "Epoch [5][10]\t Batch [50][550]\t Training Loss 0.7900\t Accuracy 0.8524\n",
      "Epoch [5][10]\t Batch [100][550]\t Training Loss 0.7920\t Accuracy 0.8470\n",
      "Epoch [5][10]\t Batch [150][550]\t Training Loss 0.7993\t Accuracy 0.8425\n",
      "Epoch [5][10]\t Batch [200][550]\t Training Loss 0.8010\t Accuracy 0.8418\n",
      "Epoch [5][10]\t Batch [250][550]\t Training Loss 0.7984\t Accuracy 0.8423\n",
      "Epoch [5][10]\t Batch [300][550]\t Training Loss 0.7962\t Accuracy 0.8431\n",
      "Epoch [5][10]\t Batch [350][550]\t Training Loss 0.8018\t Accuracy 0.8414\n",
      "Epoch [5][10]\t Batch [400][550]\t Training Loss 0.7995\t Accuracy 0.8426\n",
      "Epoch [5][10]\t Batch [450][550]\t Training Loss 0.8003\t Accuracy 0.8426\n",
      "Epoch [5][10]\t Batch [500][550]\t Training Loss 0.8001\t Accuracy 0.8423\n",
      "\n",
      "Epoch [5]\t Average training loss 0.7978\t Average training accuracy 0.8423\n",
      "Epoch [5]\t Average validation loss 0.7025\t Average validation accuracy 0.8840\n",
      "\n",
      "Epoch [6][10]\t Batch [0][550]\t Training Loss 0.7309\t Accuracy 0.8600\n",
      "Epoch [6][10]\t Batch [50][550]\t Training Loss 0.7610\t Accuracy 0.8612\n",
      "Epoch [6][10]\t Batch [100][550]\t Training Loss 0.7644\t Accuracy 0.8556\n",
      "Epoch [6][10]\t Batch [150][550]\t Training Loss 0.7723\t Accuracy 0.8505\n",
      "Epoch [6][10]\t Batch [200][550]\t Training Loss 0.7745\t Accuracy 0.8498\n",
      "Epoch [6][10]\t Batch [250][550]\t Training Loss 0.7724\t Accuracy 0.8503\n",
      "Epoch [6][10]\t Batch [300][550]\t Training Loss 0.7710\t Accuracy 0.8508\n",
      "Epoch [6][10]\t Batch [350][550]\t Training Loss 0.7769\t Accuracy 0.8496\n",
      "Epoch [6][10]\t Batch [400][550]\t Training Loss 0.7754\t Accuracy 0.8503\n",
      "Epoch [6][10]\t Batch [450][550]\t Training Loss 0.7767\t Accuracy 0.8503\n",
      "Epoch [6][10]\t Batch [500][550]\t Training Loss 0.7771\t Accuracy 0.8497\n",
      "\n",
      "Epoch [6]\t Average training loss 0.7754\t Average training accuracy 0.8495\n",
      "Epoch [6]\t Average validation loss 0.6871\t Average validation accuracy 0.8904\n",
      "\n",
      "Epoch [7][10]\t Batch [0][550]\t Training Loss 0.7165\t Accuracy 0.8800\n",
      "Epoch [7][10]\t Batch [50][550]\t Training Loss 0.7451\t Accuracy 0.8641\n",
      "Epoch [7][10]\t Batch [100][550]\t Training Loss 0.7492\t Accuracy 0.8604\n",
      "Epoch [7][10]\t Batch [150][550]\t Training Loss 0.7574\t Accuracy 0.8550\n",
      "Epoch [7][10]\t Batch [200][550]\t Training Loss 0.7600\t Accuracy 0.8539\n",
      "Epoch [7][10]\t Batch [250][550]\t Training Loss 0.7581\t Accuracy 0.8544\n",
      "Epoch [7][10]\t Batch [300][550]\t Training Loss 0.7572\t Accuracy 0.8552\n",
      "Epoch [7][10]\t Batch [350][550]\t Training Loss 0.7634\t Accuracy 0.8539\n",
      "Epoch [7][10]\t Batch [400][550]\t Training Loss 0.7624\t Accuracy 0.8547\n",
      "Epoch [7][10]\t Batch [450][550]\t Training Loss 0.7639\t Accuracy 0.8545\n",
      "Epoch [7][10]\t Batch [500][550]\t Training Loss 0.7647\t Accuracy 0.8541\n",
      "\n",
      "Epoch [7]\t Average training loss 0.7635\t Average training accuracy 0.8538\n",
      "Epoch [7]\t Average validation loss 0.6800\t Average validation accuracy 0.8950\n",
      "\n",
      "Epoch [8][10]\t Batch [0][550]\t Training Loss 0.7105\t Accuracy 0.8900\n",
      "Epoch [8][10]\t Batch [50][550]\t Training Loss 0.7371\t Accuracy 0.8684\n",
      "Epoch [8][10]\t Batch [100][550]\t Training Loss 0.7417\t Accuracy 0.8645\n",
      "Epoch [8][10]\t Batch [150][550]\t Training Loss 0.7501\t Accuracy 0.8587\n",
      "Epoch [8][10]\t Batch [200][550]\t Training Loss 0.7529\t Accuracy 0.8580\n",
      "Epoch [8][10]\t Batch [250][550]\t Training Loss 0.7512\t Accuracy 0.8584\n",
      "Epoch [8][10]\t Batch [300][550]\t Training Loss 0.7507\t Accuracy 0.8590\n",
      "Epoch [8][10]\t Batch [350][550]\t Training Loss 0.7570\t Accuracy 0.8576\n",
      "Epoch [8][10]\t Batch [400][550]\t Training Loss 0.7562\t Accuracy 0.8583\n",
      "Epoch [8][10]\t Batch [450][550]\t Training Loss 0.7580\t Accuracy 0.8580\n",
      "Epoch [8][10]\t Batch [500][550]\t Training Loss 0.7590\t Accuracy 0.8573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.7580\t Average training accuracy 0.8570\n",
      "Epoch [8]\t Average validation loss 0.6781\t Average validation accuracy 0.8972\n",
      "\n",
      "Epoch [9][10]\t Batch [0][550]\t Training Loss 0.7091\t Accuracy 0.8900\n",
      "Epoch [9][10]\t Batch [50][550]\t Training Loss 0.7343\t Accuracy 0.8702\n",
      "Epoch [9][10]\t Batch [100][550]\t Training Loss 0.7392\t Accuracy 0.8666\n",
      "Epoch [9][10]\t Batch [150][550]\t Training Loss 0.7477\t Accuracy 0.8610\n",
      "Epoch [9][10]\t Batch [200][550]\t Training Loss 0.7507\t Accuracy 0.8600\n",
      "Epoch [9][10]\t Batch [250][550]\t Training Loss 0.7490\t Accuracy 0.8605\n",
      "Epoch [9][10]\t Batch [300][550]\t Training Loss 0.7488\t Accuracy 0.8612\n",
      "Epoch [9][10]\t Batch [350][550]\t Training Loss 0.7551\t Accuracy 0.8600\n",
      "Epoch [9][10]\t Batch [400][550]\t Training Loss 0.7546\t Accuracy 0.8605\n",
      "Epoch [9][10]\t Batch [450][550]\t Training Loss 0.7565\t Accuracy 0.8602\n",
      "Epoch [9][10]\t Batch [500][550]\t Training Loss 0.7576\t Accuracy 0.8594\n",
      "\n",
      "Epoch [9]\t Average training loss 0.7569\t Average training accuracy 0.8591\n",
      "Epoch [9]\t Average validation loss 0.6797\t Average validation accuracy 0.8988\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN NETWORK\n",
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.8728.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST NETWORK\n",
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
